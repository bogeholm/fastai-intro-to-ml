{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course lecture\n",
    "[3 - Performance, Validation and Model Interpretation](http://course18.fast.ai/lessonsml1/lesson3.html)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bogeholm/fastai-intro-to-ml/blob/master/3-performance-validation-and-model-interpretation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## Notes from last\n",
    "- Default settings of Random Forests in `scikit-learn` are very reasonable\n",
    "- `proc_df()` from `fast.ai` adds a Boolean `isnull` column for *numerical* values, after replacing nulls with the *median*\n",
    "- There may be values in the test set that weren't in the training set\n",
    "- The median may differ between the training and test sets\n",
    "- `proc_df()` changed to return `nas`, a dictionary with names of columns with nulls as keys, and medians as values\n",
    "- `nas` can be passed as an optional argument to the new `proc_df()`, as well as being returned from it\n",
    "\n",
    "## Main topic\n",
    "- Understanding the results of the model - Random Forests are not black boxes\n",
    "\n",
    "### Predictive confidence\n",
    "- We don't just want a prediction, we want a confidence of the prediction. Any 'unusual' rows should get low confidence\n",
    "- If the standard deviation of a row prediction between trees is high, we have low confidence\n",
    "\n",
    "### Feature importance\n",
    "- Importance can be used to find out *which features to learn more about*\n",
    "- If importance goes against expectations, it is worthwhile to investigate. Data problems? New knowledge?\n",
    "- Maybe a column is only predictive for values that are *missing*? In an example (1:11:45), information about research grant applications - the topic of interest - was only entered into a database *manually* for grant applications *that were accepted*; ie. `null => acceptance=False`. This is **data leakage**. \n",
    "- **Colinearity** is another possibility - the variable could be indicative of something else entirely. Ie. `OLED_screen=True` for TV sets would likely indicate a later manufacturing date that `plasma_screen=True` \n",
    "- By removing less important variables, you are potentially removing sources of colinearity. This makes the 'true' relationships more clear\n",
    "- If possible, inspect the variables deeper. Fuzzy variables (eg. `fiModelDesc`); can they be ordered? Can we split the strings?\n",
    "\n",
    "\n",
    "### Tips on large datasets - side topic\n",
    "- Large datasets means $\\approxeq 10^8$ rows in this particular definition. A specific example is the [Kaggle groceries competition](https://www.kaggle.com/c/favorita-grocery-sales-forecasting), featuring a relational star-schema dataset\n",
    "- Create a type dict for pandas to use while loading data, to not consume too much RAM for inferring types; eg:\n",
    "```python\n",
    "types = {'id': 'int32', 'category':'int8'}\n",
    "df = pd.read_csv('data.csv', dtype=types)\n",
    "```\n",
    "- Use UNIX `shuf` command to get a random sample of the lines in the dataset. Then work as much as possible on the sample before progressing\n",
    "- Convert as many as possible `object` columns into more primitive types (`bool`s, `int32`s, ...)\n",
    "- Random subsampling for each tree in a forest with [`set_rf_samples()`](https://github.com/fastai/fastai/blob/c655762c3dc835ea61ad9143d84f1c3b47fe60f4/old/fastai/structured.py#L398) can be useful\n",
    "- Set `min_samples_leaf=LARGISH_NUMBER` (`100` suggested at 31:00) to save time_resources\n",
    "- Do not use `oob_score=True` when using `set_rf_samples()` on very large datasets\n",
    "- RandomForests do not work well on the groceries dataset out of the box\n",
    "- Hacker tip: take the last two week of data; use store, item, on_promotion average sales - that got into the top 30 on Kaggle!\n",
    "\n",
    "### Other tips\n",
    "- Check out `pandas` function `df.describe(include='all')`\n",
    "- Check out `%prun` for profiling function calls in notebooks\n",
    "- When working with time series, you will generally have dates **later** in the data set you want to predict, than in the training set. If you want to use a subset of the training set for experimenting, use the **latest** dates possible (*in general*)\n",
    "\n",
    "\n",
    "## Further notes\n",
    "- Random Forests are fine for structured data\n",
    "- For unstructured data (images, speech, ...), try deep learning\n",
    "- Quality of validation set directly influences quality of model in production\n",
    "- Plotting \"quality\" of validation set on the `x`-axis and quality in production on `y`-axis should ideally produce a straight line\n",
    "- Coding Machine Learning is not technically difficult - but if you get a tiny detail wrong, it means your model ends up worse than it could. If you're not doing eg. Kaggle, there's not a good way to know that. This is an open problem.\n",
    "- Supplement your model using external data, if possible (historical weather data, oil price, ...).\n",
    "- Check out [Rossman store sales](https://www.kaggle.com/c/rossmann-store-sales/overview/description) on Kaggle, and [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html). See also the [Kaggle blog](https://medium.com/kaggle-blog)\n",
    "\n",
    "\n",
    "# Implementation plan\n",
    "- [ ] Determine *test* vs. *validation* set nomenclature\n",
    "- [x] Read the data you saved last time\n",
    "- [x] `min_samples_leaf=3` and `max_features=0.5` turned out fine last time. \n",
    "  - [ ] Describe what these settings do\n",
    "- [ ] Inspect the data, lookong at relative standard deviation based on categories (`std` / `prediction`)\n",
    "- [x] Validation set are the last `12_000` values of the test set\n",
    "- [ ] Take 50 trees, and add `std` to a dataFrame copy\n",
    "- [ ] Compare predictions using all variables, and only the - say - top ten\n",
    "- [ ] Use pandas plotting functions, for the sport of it\n",
    "- [ ] `rf_feat_importance` (from `fast.ai`, based on `scikit-learn`)\n",
    "- [ ] Plot ordered, barplots, whatever looks good\n",
    "- [ ] Move `split_dataset()` to [dataworks](https://github.com/bogeholm/dataworks) \n",
    "\n",
    "## Technique for inspecting feature importance\n",
    "- Start with base dataset and make a predictive model $m_0$.\n",
    "- Pick a column, and randomly shuffle it. Compare $R^2$ and RMSE of $m_0$ on the original and shuffled datasets.\n",
    "- Why not just exclude the column? That would mean training a whole new random forest for each column, which is slow.\n",
    "\n",
    "## Ideas for improvement\n",
    "- Look at `train_cats(df)` and `apply_cats(df1, df2)` which are called (in that order) before `proc_df()`\n",
    "- Keep track of missing values `in proc_df`? Return dict with missing column values, as well as medians, to use on unseen data, or data subsets\n",
    "- Use all values for training (test set), *last* 12_000 for validation\n",
    "- Parallelize! (1:00:00) Check out `parallel_trees()` from `fast.ai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install [dataworks](https://github.com/bogeholm/dataworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dataworks:\n",
    "#!pip install --upgrade --quiet git+git://github.com/bogeholm/dataworks.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get [notebook utilities file](https://github.com/bogeholm/fastai-intro-to-ml/blob/master/notebookutils.py) - only relevant if in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to fetch notebook utilities\n",
    "#!curl --proto '=https' --tlsv1.2 -sSf --output notebookutils.py 'https://raw.githubusercontent.com/bogeholm/fastai-intro-to-ml/master/notebookutils.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "#from dataworks.df_utils import (add_datefields, add_nan_columns, categorize_df, inspect_df, \n",
    "#    numeric_nans, summarize_df,)\n",
    "\n",
    "from notebookutils import get_basepath, rmse, print_score\n",
    "from typing import Optional, List, Tuple\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# POandas options\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "# Matplotlib/Jupyter extras and options\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('png')\n",
    "# https://matplotlib.org/3.1.1/gallery/style_sheets/style_sheets_reference.html\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive (for [Colab](https://colab.research.google.com/) integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEPATH = get_basepath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAPATH: C:\\Users\\tbm\\Documents\\machinelearning\\fastai-intro-to-ml\\data\\bulldozers\n"
     ]
    }
   ],
   "source": [
    "DATAPATH = os.path.join(*[BASEPATH, 'bulldozers'])\n",
    "print(f'DATAPATH: {DATAPATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = os.path.join(*[DATAPATH, 'Train-Preprocessed.zip'])\n",
    "df_all = pd.read_csv(DATASET, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let'scheck if we got the log of sale price\n",
    "'LogSalePrice' in df_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_allrows(df):\n",
    "    \"\"\" Override max rows and display them all\n",
    "    \"\"\"\n",
    "    with pd.option_context('display.max_rows', len(df)):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df: pd.DataFrame, column: str, indices: Optional[List[bool]]=None, \n",
    "                  fromto: Optional[Tuple[int, int]]=None, length: Optional[int]=None) -> Tuple[pd.DataFrame, np.array]:\n",
    "    \"\"\" Split DataFrame into dependent and independent variables. \n",
    "        'indices' overrides 'fromto', 'fromto' overrides 'length'\n",
    "    \"\"\"\n",
    "    if indices is not None:\n",
    "        resdf = df[indices].copy(deep=True)\n",
    "    elif fromto is not None:\n",
    "        (low, up) = fromto\n",
    "        resdf = df[low:up].copy(deep=True)\n",
    "    elif length is not None:\n",
    "        resdf = df[0:length].copy(deep=True)\n",
    "    else:\n",
    "        resdf = df.copy(deep=True)\n",
    "    \n",
    "    # dependent variable\n",
    "    y_vals = np.array(resdf[column].values)\n",
    "    # independent variable(s)\n",
    "    resdf.drop(columns=column, inplace=True)\n",
    "    \n",
    "    return resdf, y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train: 10000\n",
      "n_valid: 5000\n"
     ]
    }
   ],
   "source": [
    "len(df_all)\n",
    "small_dataset = True\n",
    "\n",
    "n_train = 10_000 if small_dataset else len(df_all) - 12_000\n",
    "n_valid = 5_000 if small_dataset else 12_000\n",
    "assert n_train + n_valid <= len(df_all), f'Im sorry, I cannot find {n_train+n_valid} data points'\n",
    "\n",
    "print(f'n_train: {n_train}')\n",
    "print(f'n_valid: {n_valid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = split_dataset(df_all, 'LogSalePrice', fromto=(0, n_train))\n",
    "x_valid, y_valid = split_dataset(df_all, 'LogSalePrice', fromto=(n_train, n_train+n_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(max_depth=3, max_features=0.5, n_estimators=50, n_jobs=-1, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 734 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=3,\n",
       "                      max_features=0.5, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "                      oob_score=True, random_state=None, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:    0.531\n",
      "RMSE:   0.475\n"
     ]
    }
   ],
   "source": [
    "print_score(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for est in model.estimators_:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

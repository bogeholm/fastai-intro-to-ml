{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course lecture\n",
    "[3 - Performance, Validation and Model Interpretation](http://course18.fast.ai/lessonsml1/lesson3.html)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bogeholm/fastai-intro-to-ml/blob/master/3-performance-validation-and-model-interpretation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## Notes from last\n",
    "- Defaults of Random Forests in `scikit-learn` are very reasonable\n",
    "- `proc_df()` from `fast.ai` add a Boolean null column from *numerical* values, after replacing nulls with the **median**\n",
    "  - There may be values in the test set that weren't in the training set\n",
    "  - The median may differ between the training and test sets\n",
    "  - `proc_df()` changed to return `nas`, a dictionary with names of columns with nulls as keys, and medians as values\n",
    "  - `nas` can be passed as an optional argument to the new `proc_df()`\n",
    "\n",
    "## Today\n",
    "- Understanding the results of the model - Random Forests are not black boxes\n",
    "- Working with large datasets ($> 10^8$ rows), specifically the [Kaggle groceries competition](https://www.kaggle.com/c/favorita-grocery-sales-forecasting), a relational, start-schema dataset\n",
    "- When working with time series, you will generally have dates **later** in the test set, than in the training set. If you want to use a subset of the training set for experimenting, use the **latest** dates possible (*in general*)\n",
    "\n",
    "### Tips on large datasets\n",
    "- Create a type dict for pandas to use while loading data, to not consume too much RAM for inferring types; eg:\n",
    "```python\n",
    "types = {'id': 'int32', 'category':'int8'}\n",
    "df = pd.read_csv('data.csv', dtype=types)\n",
    "```\n",
    "- Use UNIX `shuf` command to get a random sample of the lines in the dataset. Then work as much as possible on the sample before progressing\n",
    "- Convert as many as possible `object` columns into more primitive types (`bool`s, `int32`s, ...)\n",
    "- Consider `set_rf_samples()` again\n",
    "- Set `min_samples_leaf=100` (31:00)\n",
    "- Do not use `oob_score=True` when using `set_rf_samples` on very large datasets\n",
    "\n",
    "### Other tips\n",
    "- pandas `df.describe(include='all')`\n",
    "- Check out `%prun` for profiling function calls in notebooks\n",
    "\n",
    "## Background\n",
    "- Random Forests are fine for structured data\n",
    "- For unstructured data (images, speech, ...), try deep learning\n",
    "- Quality of validation set directly influences quality of model in production\n",
    "- Plotting \"quality\" on validation set on x-axis and quality in production on y-axis should ideally lie on a straight line\n",
    "\n",
    "## Further notes\n",
    "- Look at `train_cats(df)` and `apply_cats(df1, df2)` which are called (in that order) before `proc_df()`\n",
    "- RandomForests do not work well on the groceries dataset out of the box\n",
    "- Tip: take the last two week of data; use store, item, on_promotion average sales - that got into the top 30 on Kaggle!\n",
    "- Coding Machine Lerning is not technically difficult - but if you get a tiny detail wrong, it means your model ends up worse than it could. If you're not doing eg. Kaggle, there's not a good way to know that. This is an open problem.\n",
    "- Supplement using external data, if possible.\n",
    "- Check out [Rossman store sales](https://www.kaggle.com/c/rossmann-store-sales/overview/description) on Kaggle, and [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html). See also the [Kaggle blog](https://medium.com/kaggle-blog)\n",
    "\n",
    "\n",
    "# Implementation plan\n",
    "- Determine *test* vs. *validation* set nomenclature\n",
    "- Clean up notes\n",
    "- Read the data you saved last time\n",
    "- Keep track of missing values `in proc_df`? Return dict with missing column values, as well as medians, to use on unseen data, or data subsets\n",
    "- Use all values for training (test set), *last* 12_000 for validation\n",
    "\n",
    "## Implementation notes\n",
    "- We don't just want a prediction, we want a confidence of the prediction. Any 'unusual' rows should get low confidence\n",
    "- If the standard deviation of a row prediction between trees is high, we have low confidence\n",
    "- `min_samples_leaf=3` and `max_features=0.5` turned out fine last time. Show?\n",
    "- Parallelize! (1:00:00) `parallel_trees()` from `fast.ai`\n",
    "- Inspect the data, lookong at relative standard deviation based on categories (`std` / `prediction`)\n",
    "- Compare predictions using all variables, and only the - say - top ten\n",
    "- Use pandas plotting functions, for the sport of it\n",
    "- Possibly learn more about the variables. Fuzzy variables (eg. `fiModelDesc`); can they be ordered? Can we split the strings?\n",
    "\n",
    "### Feature importance\n",
    "- `rf_feat_importance` (from `fast.ai`, based on `scikit-learn`)\n",
    "  - Plot ordered, barplots, however it looks good\n",
    "  - Importance can be used to find out *which features to learn more about*\n",
    "  - If importance goes against expectations, it is worthwhile to investigate. Data problems? New knowledge?\n",
    "  - Maybe a column is only predictive for values that are *missing* (example @ 1:11:45)? In this example, information about research grant applications (the topic of interest) was only entered into a database *manually* for grant applications *that were accepted*; ie. `null => acceptance=False`. This is **data leakage**. \n",
    "  - **Colinearity** is another possibility - the variable could be indicative of something else entirely\n",
    "  - By removing less important variables, you are potentially removing sources of colinearity. This makes the 'true' relationships more clear\n",
    "\n",
    "### Technique for inspecting feature importance\n",
    "- Start with base dataset and make a predictive model $m_0$.\n",
    "- Pick a column, and randomly shuffle it. Compare $R^2$ and RMSE of $m_0$ on the original and shuffled datasets.\n",
    "- Why not just exclude the column? That would mean training a whole new random forest for each column, which is slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install [dataworks](https://github.com/bogeholm/dataworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dataworks:\n",
    "#!pip install --upgrade --quiet git+git://github.com/bogeholm/dataworks.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get [notebook utilities file](https://github.com/bogeholm/fastai-intro-to-ml/blob/master/notebookutils.py) - only relevant if in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to fetch notebook utilities\n",
    "#!curl --proto '=https' --tlsv1.2 -sSf --output notebookutils.py 'https://raw.githubusercontent.com/bogeholm/fastai-intro-to-ml/master/notebookutils.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "#from dataworks.df_utils import (add_datefields, add_nan_columns, categorize_df, inspect_df, \n",
    "#    numeric_nans, summarize_df,)\n",
    "\n",
    "from notebookutils import get_basepath, rmse, print_score\n",
    "\n",
    "# POandas options\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "# Matplotlib/Jupyter extras and options\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('png')\n",
    "# https://matplotlib.org/3.1.1/gallery/style_sheets/style_sheets_reference.html\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive (for [Colab](https://colab.research.google.com/) integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEPATH = get_basepath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAPATH: C:\\Users\\tbm\\Documents\\machinelearning\\fastai-intro-to-ml\\data\\all\\your\\sub\\directories\n"
     ]
    }
   ],
   "source": [
    "DATAPATH = os.path.join(*[BASEPATH, 'all', 'your', 'sub', 'directories'])\n",
    "print(f'DATAPATH: {DATAPATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_allrows(df):\n",
    "    \"\"\" Override max rows and display them all\n",
    "    \"\"\"\n",
    "    with pd.option_context('display.max_rows', len(df)):\n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start learning ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

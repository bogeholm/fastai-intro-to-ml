{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course lecture\n",
    "[2 - Random Forest Deep Dive](http://course18.fast.ai/lessonsml1/lesson2.html)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bogeholm/fastai-intro-to-ml/blob/master/2-random-forest-deep-dive.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## Lessons from last\n",
    "- Learn the metric (RMSE log sale price)\n",
    "- All columns should be _numbers_\n",
    "  - `datetime` -> `bool`eans (`dayofweek`, `dayofyear`, ...)\n",
    "  - All string values must be categorized\n",
    "- Missing values replaced by median\n",
    "- Additional boolean column `f'{colname}_na'` added to indicate missing values\n",
    "\n",
    "## This time\n",
    "- Discussion of [$R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination) in the context of [overfitting](https://en.wikipedia.org/wiki/Overfitting) and the [validation set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets)\n",
    "- Validation sets vs. training sets - **test set** is reserved for validating model after training and choosing hyperparameters on the **validation set** (*unclear which set is used for which - ask Google*)\n",
    "- If the test set represents a different time period than the training set, so should the validation set\n",
    "\n",
    "### Speeding things up\n",
    "- Choose a smaller subset of the data for interactive use\n",
    "- Build model with sinle tree (32:00)\n",
    "\n",
    "### Random Forests\n",
    "- A tree consists of a set of binary decisions\n",
    "- How do you split the trees?\n",
    "   - Eg. try every possible split of every possible value, and find the one with the best weighted average of MSE in the two groups\n",
    " - The tree is finished when there is only one element in each leaf node (default `scikit-learn` behavior)\n",
    "- A *Forest* is made of *Trees*. The Forest is made of Trees by Bagging. See \"Bag of Little Bootstraps\", a method of *ensembling*.\n",
    "- Method: Create a largish set of trees, that massively overfits on a random subset of the data. They all have random errors. What is the average of a set of random errors?\n",
    "- `scikit-learn` pick random subsets with replacement, ie. Bootstrapping.\n",
    "- The goal of Random Forests is to come up with 'predictive, but poorly correlated trees' (find link to original 1990's paper)\n",
    "- Un-correlated trees is more important than accurate trees - see `scikit-learn`'s `XtraTreesRegresor`\n",
    "- When turning off bootstrapping (`bootstrap=False`), the shallow tree will be contained in a deeper tree\n",
    "\n",
    "### Hyperparameters\n",
    "#### Number of trees\n",
    "- `scikit-learn` parameter `n_estimators`\n",
    "\n",
    "#### Out-of-bag score\n",
    "- At each level, use unused rows a validation sets (`oob_score=True` in `scikit-learn`) (1:12:00)\n",
    "\n",
    "#### Subsampling\n",
    "- If you pick a random subset for each tree, it doesn't matter how much data you have\n",
    "\n",
    "#### Growing trees less deeply\n",
    "- `scikit-learn` parameter `min_samples_leaf` - determines the minimum number of data points in the leaves. `3` suggested.\n",
    "\n",
    "#### Maximum number of features\n",
    "- `scikit-learn` parameter `max_features` The less correlated your trees are, the better. Takes a different subset of columns at each split point. Try `0.5`, `sqrt` or `log2`\n",
    "\n",
    "## Implementation plan\n",
    "- **Can you fit on a DataFrame with categories added, but columns still in place?**\n",
    "- Split data to smaller, 'interactive' set\n",
    "- Draw a single tree\n",
    "- Run the `RandomForestRegressor`, see predictions of the single trees ('`estimators`') (51:16)\n",
    "- Define `print_score()` with `oob_score`\n",
    "- Predict with each tree in a Forest, and compare with the mean (1:05)\n",
    "- Plot metrics of one tree, then average of two trees, then ... (1:06:55)\n",
    "- Run with 20, 30, 40 trees (1:08:10)\n",
    "- Look at `set_rf_samples` and `reset_rf_samples` (from `fast.ai`, a \"horrible hack\") (1:16:00 and 1:19)\n",
    "...\n",
    "- Compare results with `XtraTreesRegressor`\n",
    "\n",
    "\n",
    "# Tips\n",
    "## Notebook tips\n",
    "- `?` to view documentation of imported functions\n",
    "- `??` to view source code of imported functions\n",
    "\n",
    "## Python tips\n",
    "- `print(f'{variable}_string')`\n",
    "- Saving with [`to_feather`](https://github.com/wesm/feather/tree/master/python)\n",
    "\n",
    "## ML Tips\n",
    "- \"Most people run al of thei models, on all of their data, all of the time\" (around 1:20). This is pointless. Do most of the modelling on a reasonably large subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Install [dataworks](https://github.com/bogeholm/dataworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install utilities:\n",
    "!pip install --upgrade --quiet git+git://github.com/bogeholm/dataworks.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from dataworks.df_utils import (add_datefields, \n",
    "                                add_nan_columns, \n",
    "                                categorize_df,\n",
    "                                inspect_df, \n",
    "                                numeric_nans, \n",
    "                                summarize_df, \n",
    "                                )\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive (for [Colab](https://colab.research.google.com/) integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basepath(relative_dir_local='data/', relative_dir_google='data/'):\n",
    "    \"\"\" Return path to base directory depending on whether the\n",
    "        notebook is running locally, or in Google Colab. If the notebook\n",
    "        is running in Colab, data is loaded from Google Drive\n",
    "    \"\"\"\n",
    "    GOOGLE_DRIVE_HOME = 'drive/My Drive/' # Equivalent to `cd ~` in Google Drive\n",
    "    # https://stackoverflow.com/questions/39125532/file-does-not-exist-in-jupyter-notebook\n",
    "    JUPYTER_CWD =  os.path.dirname(os.path.abspath(''))\n",
    "    \n",
    "    if 'google.colab' in sys.modules:\n",
    "        # Notebook is running in Google Colab\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        return GOOGLE_DRIVE_HOME + relative_dir_google\n",
    "    else:\n",
    "        return JUPYTER_CWD + '/' + relative_dir_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAPATH: /Users/bogeholm/Dropbox/Code-Dropbox/Colab-FastAI/data/\n"
     ]
    }
   ],
   "source": [
    "DATAPATH = get_basepath() + ''\n",
    "print(f'DATAPATH: {DATAPATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_allrows(df):\n",
    "    \"\"\" Override max rows and display them all\n",
    "    \"\"\"\n",
    "    with pd.option_context('display.max_rows', len(df)):\n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start learning ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
